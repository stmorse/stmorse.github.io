---
layout: post
title: "Elo as a statistical learning model"
categories: journal
date: 2019-05-18
tags: [projects, football, machine learning]
---

Elo ratings are a ubiquitous system for ranking individuals/teams who compete in pairs.  They began as a chess rating system, designed by Arpad Elo, but have since spread to sports and elsewhere (here's an application in education).  As a result, there is a lot of info out there on how to compute Elo ratings, presumably because they're easy to understand and surprisingly good at forecasting.  Heck, pretty much all of Nate Silver's 538 sports coverage revolves around it.

But there are very, very few resources on **how** the ratings work.  I don't mean what the formula is, I mean **why** the formula is what it is.  What is the deeper statistical truth going on here?

I'm going to take a schwack at this.  I'll explore the Elo ratings as weights of a logistic regression found through stochastic gradient descent.  A few other resources mention or hint at this interpretation, but their explanations feel cursory at best to me.  This also helps understand Elo ratings as a Markov process, since SGD is itself interpretable as a stochastic process.  

My hope is that placing Elo in a statistical learning framework will allow us to extend it, or improve it.

**CAVEAT:**  An ulterior motive of this post is to lure the casual sports analytics enthusiast into a deeper understanding of stats, so if you have a PhD in statistics and a lot of this seems pedantic ... my bad.  (Also, if you find errors ... let me know!)


## Elo: a quick primer

Let's say Team A and Team B have Elo ratings $$\text{elo}_A$$ and $$\text{elo}_B$$.  They play a game against each other.  This results in the update

$$
\begin{equation}
\text{elo}_{i}^{(\text{new})} = \text{elo}_{i}^{(\text{old})} + k (t_{ij} - \text{Pr}(i \text{ beats } j))
\label{eq:elo}
\end{equation}
$$

where $$k$$ is the "k"-factor, $$t_{ij} = 1$$ if $$i$$ beats $$j$$ (0 otherwise) is the game outcome, and $$\text{Pr}(i \text{ beats } j)$$ is the probability that $$i$$ wins the game.  The k-factor captures how much impact a single game has on the ranking, so for example, 538 uses 20 for NFL games and 4 for MLB games.  The probability of winning is defined as a weird base-10 sigmoid function of the difference in ratings:

$$
\text{Pr}(i \text{ beats } j) = \frac{1}{1+10^{-(\text{elo}_i - \text{elo}_j)/400}}
$$

That 400 is typically described as a "scale factor" (but why 400?).  The base 10 comes because, for whatever reason, Arpad Elo decided to use $$\log_{10}$$ odds instead of the typically $$\log_e$$ odds.  That is,

$$
$$

This allows us to compute predictions.  


## Logistic regression

This should all remind us of logistic regression.  As a quick and dirty review, let's say we have some data $$\mathcal{D}=\{(\mathbf{x}_i, y_i)\}$$ where $$\mathbf{x}_i\in\mathbb{R}^d$$ and $$y_i\in\{0,1\}$$.  We are interested in predicting an outcome $$y$$ given input $$\mathbf{x}$$.   

First define the "log-odds"

$$
a = \log \frac{\text{Pr}(\mathbf{x}| y=1)\text{Pr}(y=1)}{\text{Pr}(\mathbf{x}| y=1)\text{Pr}(y=1)}
$$

where we typically assume $$\log = \log_e$$.  Then note, by Bayes' theorem,

$$
\begin{align*}
\text{Pr}(y=1|\mathbf{x}) &= \frac{\text{Pr}(\mathbf{x}| y=1)\text{Pr}(y=1)}{\sum_k \text{Pr}(\mathbf{x}| y=k)\text{Pr}(y=k)} \\
&= \frac{1}{1+\frac{\text{Pr}(\mathbf{x}| y=2)\text{Pr}(y=2)}{\text{Pr}(\mathbf{x}| y=1)\text{Pr}(y=1)}} \\
&= \frac{1}{1+e^{-a}}
\end{align*}
$$

This is the workhorse "sigmoid" function, usually written $$\sigma(a)=1/(1+\text{exp}(-a)$$.  If we now assume the log-odds can be written as a linear combination of the inputs $$\mathbf{x}$$ (or more probabilistically, if we assume the conditional densities are Gaussian with a shared covariance matrix), then

$$
\text{Pr}(y=1 | \mathbf{x}; \mathbf{w}) = \sigma(\mathbf{w}^T \mathbf{x})
$$

for some "weight" vector $$\mathbf{w}$$.  Then the data likelihood is

$$
\text{Pr}(\mathcal{D}|\mathbf{w}) = \prod_{i=1}^N \sigma^{y_i} (1-\sigma)^{1-y_i}
$$

which we'd like to maximize, or equivalently the negative log-likelihood

$$
E(\mathbf{w}) := -\log \text{Pr}(\mathcal{D}|\mathbf{w}) = -\sum_{i=1}^N y_i \ln \sigma + (1-y_i) \ln (1-\sigma)
$$

called the "cross-entropy," which we'd like to minimize. (The entropy would just be the sum over the first of those two terms.)  Since $$\partial_{\mathbf{w}} \sigma = \sigma (1-\sigma)$$ (so nice!), the gradient turns out to be

$$
\begin{equation}
\nabla_{\mathbf{w}} E = \sum_{i=1}^N (\sigma(\mathbf{w}^T \mathbf{x}_i) - t_i) \mathbf{x}_i
\label{eq:entropy_grad}
\end{equation}
$$

which you should verify on a piece of scrap paper sometime when you're feeling frisky.

There isn't a closed-form solution to Eq. \eqref{eq:entropy_grad} like there is for, say, linear regression, but we can readily apply a numerical technique like gradient descent.


## Stochastic Gradient Descent

Gradient descent is the beautifully simple technique of looking for a minimum by taking small steps in the direction of the negative gradient, that is,

$$
\begin{equation}
\mathbf{w}_{k+1} = \mathbf{w}_k - \alpha_k \nabla E(\mathbf{w}_k)
\end{equation}
$$

where $$\alpha_k$$ controls how fast we update our estimate of$$\mathbf{w}$$.  Crucially, note it may be constant or adaptive (i.e. depend on $$k$$).  (Storytime: "first order" methods like gradient descent --- so-called because they only use the first order derivatives, as opposed to second-order methods like Newton-Raphson --- were once dismissed as too simplistic and slow, but found new life in the age of big data (we need it to be simple!) and fast computers (not slow anymore!).)

Now notice, when the dataset is very large, the sum happening in $$\nabla E$$ is very large, and so we are evaluating a very large function at every step of the algorithm. 

It turns out, you can just evaluate the gradient at **one** of the terms in the sum, and move in that direction, and you will still provably reach the minimum, so long as you keep picking random terms and you do it enough times!  This is absolutely magical to me.

This is called *stochastic* gradient descent, and so for logistic regression would look like

$$
\begin{equation}
\mathbf{w}_{k+1} = \mathbf{w}_k - \alpha_k \sigma(\mathbf{w}^T \mathbf{x}_i) - t_i) \mathbf{x}_i
\label{eq:SGD}
\end{equation}
$$


## A connection

Let's connect Elo to all this.  For a concrete running example, consider the NFL.  The weight vector $$\mathbf{w}$$ is now the ratings of all 32 teams.  A datapoint $$(\mathbf{x}_{(ij}), y_{(ij)})$$ can represent a specific game between teams $$i$$ and $$j$$, which we can encode as 

$$
\mathbf{x}_{(ij)} = [x_k] = \begin{cases}
1 & k=i \\
-1 & k=j \\
0 & \text{otherwise}
\end{cases}
$$

and $$y_{(ij)} = 1$$ if $$i$$ wins, 0 if $$j$$ wins.  The class conditional probability of $$i$$ winning is then

$$
\text{Pr}(y_{(ij)}=1 | \mathbf{x}_{(ij)}; \mathbf{w}_) = \sigma(\mathbf{w}^T \mathbf{x})
$$

since $$\mathbf{w}^T \mathbf{x} = w_i - w_j$$ or in Elo notation, $$\text{elo}_i - \text{elo}_j$$.  Then given some initial estimate of the weight vector $$\mathbf{w}_0$$, we can hone our estimate of the weights by iteratively applying an SGD step on the data,

$$
\begin{align*}
\mathbf{w}_{k+1} &= \mathbf{w}_k - \alpha (\sigma (\mathbf{w}^T \mathbf{x}) - 1) \mathbf{x} \\
\begin{bmatrix} ... \\ w_i \\ w_j \\ ... \end{bmatrix}_{k+1} &= \begin{bmatrix} ... \\ w_i \\ w_j \\ ... \end{bmatrix}_{k} + \alpha (1 - \frac{1}{1+e^{-(w_i - w_j)}}) \begin{bmatrix} ... \\ 1 \\ -1 \\ ... \end{bmatrix}
\end{align*}
$$

which you'll notice corresponds to the Elo update equations.  (Lots of signs flipping back and forth but it's the same!)


<!---

### Some elo mods

The update rule in Equation \eqref{eq:elo} makes intuitive sense, without any statistical background, as all beautiful simple effective things do.  In related news, it is easy for anyone with some domain knowledge to take Eq. \eqref{eq:elo} and modify it, hack it, or otherwise church it up.  Here's a few common examples:

**Home team advantage.**  In many sports, there is a measurable advantage to being the home team.  For example, in the NFL, "home team advantage" historically corresponds to about a 2.5 point bump.  Since 2.5 points corresponds to 65 Elo points, we can account for home team advantage by just adding 65 to the home team's Elo.

**Margin of victory.**  In many games there is a clear, quantifiable margin of victory that feels like it should affect the ranking update.  For example, if Team A wins by 50, their ranking should go up more than if they eked out a 1 points victory, all else held equal.  So, for example, 538 multiplies their k-factor by the following term:

$$
\begin{equation}
M = \ln (1+|\text{pts}_A - \text{pts}_B|)
\end{equation} 
$$

**Autocorrelation.**


--->

## Next up

This is a working post.  I'll close with some comments/connections to other common hacks to the Elo update equations, such as incorporating margin of victory (in the language of optimization, "side information"), and handling autocorrelation, which takes us into the realm of stochastic processes.





